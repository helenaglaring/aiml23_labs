{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Lab 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Hand-Written Digit Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as df\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, recall_score, precision_score # For evaluating model performance\n",
    "\n",
    "# Hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import Digits Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>See what they look like</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=5, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Training: %i' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 0</h1>\n",
    "\n",
    "Split the digits.data and digits.target into train and test data. Create a dummy classifier for the digits data, with a strategy of \"most_frequent\", and print the score on test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the features vector X and the target vector y.\n",
    "\n",
    "# Target vector\n",
    "y = digits.target\n",
    "\n",
    "# Features matrix\n",
    "X = digits.data\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining dummy classifier for the digits data with strategy of 'most_frequent'\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "# Fitting the model to the training data\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting y\n",
    "y_dummy_clf_pred = dummy_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Scores ----------\n",
      "Accuracy-score on training set: 0.107\n",
      "Accuracy-score on test set: 0.078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reporting scores on training and testing set \n",
    "print(\"---------- Scores ----------\")\n",
    "print(\"Accuracy-score on training set: {:.3f}\".format(dummy_clf.score(X_train, y_train)))\n",
    "print(\"Accuracy-score on test set: {:.3f}\\n\".format(dummy_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 1</h1>\n",
    "Create a logistic regression model for the digits data and print the score on test data. Use metrics.classification_report to give a more detailed report of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 1.000\n",
      "Accuracy on testing set: 0.972\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy on training set: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(logr\u001b[39m.\u001b[39mscore(X_train, y_train)))\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy on testing set: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(logr\u001b[39m.\u001b[39mscore(X_test, y_test)))\n\u001b[0;32m---> 15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPrecision-score \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m precision_score(y_test, y_logr_pred))\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mRecall-score \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m recall_score(y_test, y_logr_pred))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1776\u001b[0m, in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprecision_score\u001b[39m(\n\u001b[1;32m   1648\u001b[0m     y_true,\n\u001b[1;32m   1649\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1655\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1656\u001b[0m ):\n\u001b[1;32m   1657\u001b[0m     \u001b[39m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[1;32m   1658\u001b[0m \n\u001b[1;32m   1659\u001b[0m \u001b[39m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[39m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1776\u001b[0m     p, _, _, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[1;32m   1777\u001b[0m         y_true,\n\u001b[1;32m   1778\u001b[0m         y_pred,\n\u001b[1;32m   1779\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1780\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   1781\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   1782\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mprecision\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[1;32m   1783\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1784\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   1785\u001b[0m     )\n\u001b[1;32m   1786\u001b[0m     \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1563\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[39mif\u001b[39;00m beta \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1562\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbeta should be >=0 in the F-beta score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1563\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[1;32m   1565\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1566\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1381\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1380\u001b[0m             average_options\u001b[39m.\u001b[39mremove(\u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1381\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1382\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTarget is \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m but average=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. Please \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mchoose another average setting, one of \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (y_type, average_options)\n\u001b[1;32m   1384\u001b[0m         )\n\u001b[1;32m   1385\u001b[0m \u001b[39melif\u001b[39;00m pos_label \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39m1\u001b[39m):\n\u001b[1;32m   1386\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1387\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNote that pos_label (set to \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) is ignored when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maverage != \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (got \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m). You may use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[39mUserWarning\u001b[39;00m,\n\u001b[1;32m   1392\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "# Building initial Logistic Regression Model - all parameters not specified are set to their defaults\n",
    "logr = LogisticRegression(max_iter=10000, random_state = 42)\n",
    "\n",
    "# Train - Fitting the model to training set\n",
    "logr = logr.fit(X_train, y_train)\n",
    "\n",
    "# Predict target values (y) for new data in test set\n",
    "y_logr_pred = logr.predict(X_test)\n",
    "\n",
    "# -- Classification report -- \n",
    "# Reporting scores on training and testing set \n",
    "print(\"Accuracy on training set: {:.3f}\".format(logr.score(X_train, y_train)))\n",
    "print(\"Accuracy on testing set: {:.3f}\\n\".format(logr.score(X_test, y_test)))\n",
    "\n",
    "print('Precision-score %.3f' % precision_score(y_test, y_logr_pred))\n",
    "print('Recall-score %.3f' % recall_score(y_test, y_logr_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        33\n",
      "           1     0.9655    1.0000    0.9825        28\n",
      "           2     0.9706    1.0000    0.9851        33\n",
      "           3     0.9706    0.9706    0.9706        34\n",
      "           4     1.0000    0.9783    0.9890        46\n",
      "           5     0.9167    0.9362    0.9263        47\n",
      "           6     0.9714    0.9714    0.9714        35\n",
      "           7     1.0000    0.9706    0.9851        34\n",
      "           8     0.9667    0.9667    0.9667        30\n",
      "           9     0.9744    0.9500    0.9620        40\n",
      "\n",
      "    accuracy                         0.9722       360\n",
      "   macro avg     0.9736    0.9744    0.9739       360\n",
      "weighted avg     0.9726    0.9722    0.9723       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification report after tuning hyperparameters\n",
    "print(classification_report(y_test,y_logr_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 2</h1>\n",
    "Create a 3-way split of the data, using train_test_split. First split into trainval and test, and then split trainval into train and val. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the first step we will split the data in training and remaining dataset\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.8)\n",
    "\n",
    "# Now since we want the valid and test size to be equal (10% each of overall data). \n",
    "# we have to define valid_size=0.5 (that is 50% of remaining data)\n",
    "test_size = 0.5\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 3</h1>\n",
    "Create a logistic regression classifier for the digits data. Write a loop that tries at least 4 values ranging from .001 to 1.0 for C. Use the val data to determine the best value. Then train on the combined train and val data, and score the resulting model on test. Use metrics.ConfusionMatrixDisplay.from_predictions to display a confusion matrix for all 10 classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying RandomizedSearchCV for hyperparameter optimization for C\n",
    "\n",
    "# model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# parameter grid\n",
    "parameters = {\n",
    "    'C'       : range(0.001, 1.0, 0.1),\n",
    "}\n",
    "# using GridSearchCV\n",
    "logr_tuned= GridSearchCV(logreg,                    # model\n",
    "                   param_grid = parameters,   # hyperparameters\n",
    "                   scoring='accuracy',        # metric for scoring\n",
    "                   cv=10)   \n",
    "\n",
    "# Fitting model with optimized parameters to training set\n",
    "logr_tuned.fit(X_train,y_train)\n",
    "\n",
    "# Identifying the most optimal hyperparameters\n",
    "logr_tuned[1].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning model with optimized hyperparameters\n",
    "logr_tuned = LogisticRegression(C= 100.0)\n",
    "\n",
    "# Train - Fitting model to training set \n",
    "logr_tuned.fit(X_train, y_train)\n",
    "\n",
    "# Predict y using test set\n",
    "y_logr_pred = logr_tuned.predict(X_test)\n",
    "\n",
    "# -- Classification report -- \n",
    "# Reporting scores on training and testing set \n",
    "print(\"Accuracy on training set: {:.3f}\".format(logr_tuned.score(X_valid, y_valid)))\n",
    "print(\"Accuracy on testing set: {:.3f}\\n\".format(logr_tuned.score(X_test, y_test)))\n",
    "\n",
    "print('Precision-score %.3f' % precision_score(y_test, y_logr_pred))\n",
    "print('Recall-score %.3f' % recall_score(y_test, y_logr_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 4</h1>\n",
    "Create a Decision Tree classifier for the digits data. Write a loop that tries values of 1 through 20 for maximum depth. Use the val data to determine the best value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 5</h1>\n",
    "Now train the best decision tree model on the combined train and val data, and score the resulting model on test.  \n",
    "Use both metrics.classification_report and metrics.ConfusionMatrixDisplay.from_predictions to display the results in detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 6</h1>\n",
    "Create a Random Forest classifier for the digits data. Use the val data to determine the best values for \n",
    "<ul>\n",
    "    <li>number of estimators (from 50 to 500 by 50s)</li>\n",
    "    <li>Max depth (from 10 to 50 by 10s)</li>\n",
    "    </ul>\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 7</h1>\n",
    "Now train the best random forest model on the combined train and val data, and score the resulting model on test.  \n",
    "\n",
    "Use both metrics.classification_report and metrics.ConfusionMatrixDisplay.from_predictions to display the results in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1>Question 8</h1>\n",
    "Create a MLP classifier for the digits data. Use the val data to determine the best values for \n",
    "hidden layer sizes. Try values from (10,10) to (210,210), increasing by 50s. You can use two nested loops for variables H1 and H2, and then assign (H1,H2) to the parameter hidden_layer_sizes.\n",
    "<p> Print the best values for the two hidden layer sizes</p>\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 9</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the best mlp model on the combined train and val data, and score the resulting model on test.  \n",
    "\n",
    "Use both metrics.classification_report and metrics.ConfusionMatrixDisplay.from_predictions to display the results in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
