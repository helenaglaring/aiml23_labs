{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.filterwarnings('ignore')\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "DATA_URL = 'https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv'\n",
    "N_ROWS = 1000 # number of rows to read from input file.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Load data\n",
    "- Use `pd.read_csv` to load data from the `DATA_URL` specified above<br>\n",
    "(**NOTE**: Specify parameters `delimiter='\\t'` and `header=None`, since this data file is a `.tsv` file without header columns)\n",
    "\n",
    "- Rename the columns (`df.rename`). The text column should renamed to 'text' and label column should be called 'label'\n",
    "- Print the top three rows\n",
    "- Print the value counts of the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Load model weights\n",
    "The HuggingFace (transformers) ecosystem allows us to build down model weights for pre-trained transformer neural networks. By passing in the name (MODEL_NAME) of the model we want to use, we can load the weights into a model object automatically. \n",
    "The same thing goes for tokenizers. Most models have different tokenization schemas, which means that we want to load the tokenizer schema that works for the particular model we specified.\n",
    "\n",
    "- Run the cell below to load the model weights and tokenization schema into the Model and Tokenizer objects.\n",
    "\n",
    "- Print the `model` object. Notice how it is made up of layers just like the neural networks we trained for image classification in a prior lab, albeit a bit more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Tokenize the data\n",
    "With our tokenizer loaded, we can now preprocess our data into the format that the BERT model expects.\n",
    "\n",
    "- Tokenize the data by using the `__call__` method of the tokenizer object<br><br>\n",
    "    - e.g., `tokenized=tokenizer(df[\"text\"].to_list())`\n",
    "    - Pass the following arguments, along with the texts\n",
    "        - `add_special_tokens=True` (Adds the [CLS] and [SEP] tokens that the BERT model expects\")\n",
    "        - `padding='longest'` (The texts have uneven length. Padding means to insert dummy tokens to make the equal)\n",
    "        - `return_attention_mask=True` (Return the attention mask expected by the BERT model)\n",
    "        - `return_tensors='pt'` (Return the input ID tensors expected by the BERT model as PyTorch tensors)\n",
    "        - `verbose=True` (Tell us what's going on)\n",
    "<br><br>\n",
    "- Print the `tokenized` object (It will be a dictionary of `Ã¬nput_ids` (as tensors) and `attention_mask` (as tensors))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Calculate embedding features\n",
    "With our data tokenized such that the BERT model can understand it, we are now ready to calculate the embeddings.\n",
    "\n",
    "- Use the function below to extract embeddings\n",
    "\n",
    "- Print the shape of the embeddings. Should be (N_ROWS, 758)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(model, tokenized):\n",
    "\n",
    "    \"\"\" Calculate BERT embeddings for a batch of sentences.\n",
    "    NOTE: Calculating BERT embeddings is a very expensive operation.\n",
    "    Particularly on CPU, it can take a long time to calculate embeddings for\n",
    "    a large batch of sentences (Max 10-20 minutes for 6K sentences).\n",
    "\n",
    "    Args:\n",
    "        model (transformers BERT model): BERT model.\n",
    "        tokenized (dict): Dictionary of tokenized sentences (input_ids and attention_mask)\n",
    "\n",
    "    Returns:\n",
    "        n-d NumPy array: BERT embeddings for the sentences in the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Getting model encodings...\")\n",
    "    # The following is a context-manager that disables gradient calculation.\n",
    "    # Disabling gradient calculation is useful for inference, when you are \n",
    "    # sure that you will not call Tensor.backward(). It will reduce memory \n",
    "    # consumption for computations that would otherwise have requires_grad=True.\n",
    "    # TLDR: calculating gradients is expensive. We don't need them for inference.\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(**tokenized)\n",
    "\n",
    "    # last_hidden_states[0] is the last hidden state of the first token of the\n",
    "    # sequence (classification token) further processed by a Linear layer and \n",
    "    # a Tanh activation function. The Linear layer weights are trained from the\n",
    "    #  next sentence prediction (classification) objective during pretraining.\n",
    "    # last_hidden_states[0].shape = (batch_size, hidden_size)\n",
    "    print(\"Returning embeddings...\")\n",
    "    return last_hidden_states[0][:,0,:].numpy()\n",
    "\n",
    "embeddings = get_bert_embeddings(model, tokenized)\n",
    "embeddings.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Train a model\n",
    "Now that we have our embeddings, it is time to use them in a machine learning model. (You can use Keras too, if you feel adventurous.)\n",
    "\n",
    "- Split the embeddings and the labels into (X_train, y_train) and (X_test, y_test) using sklearn's `train_test_split` function\n",
    "\n",
    "- Fit a Logistic Regression model on (X_train, y_train)\n",
    "\n",
    "- Evaluate the results on both the training data and the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus exercises\n",
    "As discussed, the remaining tasks are bonus tasks. You are not expected to complete these before you hand in. It is just for you own understanding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 - Compare with CountVectorizer\n",
    "\n",
    "- Split the original data using train_test_split\n",
    "- Fit a CountVectorizer to (X_train)\n",
    "- Transform X_train and X_test\n",
    "- Fit a logistic regression model to the countvectorized X_train and y_train\n",
    "- Evaluate the results on both (X_train, y_train) and (X_test, y_test)\n",
    "- Compare with the results of using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml-labs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
